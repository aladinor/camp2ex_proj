{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165d8b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import sys\n",
    "import dask\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import dask.array as da\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from re import split\n",
    "from scipy.interpolate import griddata\n",
    "from scipy.spatial import cKDTree as KDTree\n",
    "import random\n",
    "from skimage.filters import gaussian, threshold_otsu\n",
    "from skimage import measure\n",
    "from dask import delayed\n",
    "from dask_image.ndfilters import uniform_filter as uf\n",
    "from dask_image.ndmeasure import variance as varian\n",
    "import dask.dataframe as dd\n",
    "from dask_jobqueue import SLURMCluster\n",
    "from dask.distributed import Client, progress\n",
    "from datetime import datetime, timezone\n",
    "import matplotlib.patches as mpatches\n",
    "from scipy import interpolate\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sys.path.insert(1, f\"{os.path.abspath(os.path.join(os.path.abspath(''), '../'))}\")\n",
    "from src.utils import get_pars_from_ini\n",
    "# from src.ufucnt_xr import lee_filter_new_\n",
    "\n",
    "location = split(', |_|-|!', os.popen('hostname').read())[0].replace(\"\\n\", \"\")\n",
    "path_data = get_pars_from_ini(campaign='loc')[location]['path_data']\n",
    "path_proj = get_pars_from_ini(campaign='loc')[location]['path_proj']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b9d074",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dask_gd2(xx, yy, z_array, target_xi, target_yi, algorithm='cubic', **kwargs):\n",
    "    \"\"\"!\n",
    "    @brief general parallel interpolation using dask and griddata\n",
    "    @param xx 1d or 2d array of x locs where data is known\n",
    "    @param yy 1d or 2d array of x locs where data is known\n",
    "    @param z_array 1d or 2d array of x locs where data is known\n",
    "    @param target_xi 2d array (or 1d grid spacing array)\n",
    "    @param target_yi 2d array (or 1d grid spacing array)\n",
    "    \"\"\"\n",
    "    n_jobs = kwargs.pop(\"n_jobs\", 4)\n",
    "    chunk_size = kwargs.get(\"chunk_size\", int(xx.size / (n_jobs - 1)))\n",
    "    if len(target_xi.shape) < 2:\n",
    "        xxt, yyt = np.meshgrid(target_xi, target_yi)\n",
    "    elif len(target_xi.shape) > 2:\n",
    "        raise RuntimeError\n",
    "    else:\n",
    "        xxt, yyt = target_xi, target_yi\n",
    "    assert xxt.shape == yyt.shape\n",
    "    z_target = np.full(np.shape(xxt), np.nan)\n",
    "\n",
    "    # evenly mix nans into dataset.  nans mark where data is needed\n",
    "    n_splits = n_jobs * 8\n",
    "    sp_xx, sp_xxt = np.array_split(xx.flatten(), n_splits), np.array_split(xxt.flatten(), n_splits)\n",
    "    sp_yy, sp_yyt = np.array_split(yy.flatten(), n_splits), np.array_split(yyt.flatten(), n_splits)\n",
    "    sp_zz, sp_zzt = np.array_split(z_array.flatten(), n_splits), np.array_split(z_target.flatten(), n_splits)\n",
    "\n",
    "    all_x = np.concatenate(np.array((sp_xx, sp_xxt)).T.flatten())\n",
    "    all_y = np.concatenate(np.array((sp_yy, sp_yyt)).T.flatten())\n",
    "    all_z = np.concatenate(np.array((sp_zz, sp_zzt)).T.flatten())\n",
    "\n",
    "    # make dask arrays\n",
    "#     import pdb; pdb.set_trace()\n",
    "    dask_xx = da.from_array(all_x, chunks=chunk_size, name=\"dask_x\")\n",
    "    dask_yy = da.from_array(all_y, chunks=chunk_size, name=\"dask_y\")\n",
    "    dask_zz = da.from_array(all_z, chunks=chunk_size, name=\"dask_z\")\n",
    "\n",
    "    dask_valid_x1 = dask_xx[~da.isnan(dask_zz)]\n",
    "    dask_valid_y1 = dask_yy[~da.isnan(dask_zz)]\n",
    "    dask_valid_z1 = dask_zz[~da.isnan(dask_zz)]\n",
    "\n",
    "    # where to interplate to\n",
    "    dask_target_x = dask_xx[da.isnan(dask_zz)]\n",
    "    dask_target_y = dask_yy[da.isnan(dask_zz)]\n",
    "\n",
    "    # interpolate for missing values\n",
    "    zz_grid = dask_interpolate(dask_valid_x1, dask_valid_y1, dask_valid_z1, dask_target_x, dask_target_y, algorithm=algorithm, **kwargs)\n",
    "    return zz_grid.reshape(xxt.shape)\n",
    "\n",
    "\n",
    "def dask_gd2_nanfill(xx, yy, z_array, algorithm='cubic', **kwargs):\n",
    "    \"\"\"!\n",
    "    @brief 2d interpolation using dask and griddata\n",
    "    @param xx np_2darray x coord array\n",
    "    @param yy np_2darray y coord array\n",
    "    @param z_array np_2darray response vals\n",
    "    \"\"\"\n",
    "    n_jobs = kwargs.pop(\"n_jobs\", 4)\n",
    "    chunk_size = kwargs.get(\"chunk_size\", int(xx.size / (n_jobs - 1)))\n",
    "    # make dask arrays\n",
    "    dask_xyz = da.from_array((xx, yy, z_array), chunks=(3, chunk_size, \"auto\"), name=\"dask_all\")\n",
    "    dask_xx = dask_xyz[0,:,:]\n",
    "    dask_yy = dask_xyz[1,:,:]\n",
    "    dask_zz = dask_xyz[2,:,:]\n",
    "\n",
    "    # select only valid values\n",
    "    dask_valid_x1 = dask_xx[~da.isnan(dask_zz)]\n",
    "    dask_valid_y1 = dask_yy[~da.isnan(dask_zz)]\n",
    "    dask_valid_z1 = dask_zz[~da.isnan(dask_zz)]\n",
    "\n",
    "    # interpolate for missing values\n",
    "    return dask_interpolate(dask_valid_x1, dask_valid_y1, dask_valid_z1, dask_xx, dask_yy, algorithm=algorithm, **kwargs)\n",
    "\n",
    "\n",
    "def dask_interpolate(dask_valid_x1, dask_valid_y1, dask_valid_z1, dask_xx, dask_yy, algorithm='cubic', vis_out='dask_par.png'):\n",
    "    # gd_chunked = [delayed(rbf_wrapped)(x1, y1, newarr, xx, yy) for \\\n",
    "    gd_chunked = [delayed(gd_wrapped)(x1.flatten(), y1.flatten(), newarr.flatten(), xx, yy, algorithm) for \\\n",
    "                x1, y1, newarr, xx, yy \\\n",
    "                in \\\n",
    "                zip(dask_valid_x1.to_delayed().flatten(),\n",
    "                    dask_valid_y1.to_delayed().flatten(),\n",
    "                    dask_valid_z1.to_delayed().flatten(),\n",
    "                    dask_xx.to_delayed().flatten(),\n",
    "                    dask_yy.to_delayed().flatten())]\n",
    "    gd_out = delayed(da.concatenate)(gd_chunked, axis=0)\n",
    "    print(gd_out)\n",
    "#     gd_out.visualize(vis_out)\n",
    "    gd1 = np.array(gd_out.compute())\n",
    "    print(gd1)\n",
    "    print(gd1.shape)\n",
    "\n",
    "    # prove we have no more nans in the data\n",
    "    assert ~np.isnan(np.sum(gd1))\n",
    "    return gd1\n",
    "\n",
    "\n",
    "def rbf_wrapped(x1, y1, newarr, xr, yr):\n",
    "    print(\"local x.size: \", x1.size)\n",
    "    rbf_interpolant = interpolate.Rbf(x1, y1, newarr, function='linear')\n",
    "    return rbf_interpolant(xr, yr)\n",
    "\n",
    "\n",
    "def gd_wrapped(x, y, v, xp, yp, algorithm='cubic', extrapolate=True):\n",
    "    # source: https://programtalk.com/python-examples/scipy.interpolate.griddata.ravel/\n",
    "    print(\"local x.size: \", x.size)\n",
    "    if x.size == 0 or xp.size == 0:\n",
    "        # nothing to do\n",
    "        return np.array([[]])\n",
    "    if algorithm not in ['cubic', 'linear', 'nearest']:\n",
    "        raise ValueError(\"Invalid interpolation algorithm: \" + str(algorithm))\n",
    "    # known data (x, y, v)  can be either 1d or 2d arrays of same size\n",
    "    # target grid: (xp, yp), xp, yp must be 2d arrays of the same shape\n",
    "    grid = interpolate.griddata((x, y), v, (xp, yp),\n",
    "                                method=algorithm, rescale=True)\n",
    "    if extrapolate and algorithm != 'nearest' and np.any(np.isnan(grid)):\n",
    "        grid = extrapolate_nans(xp, yp, grid)\n",
    "    return grid\n",
    "\n",
    "\n",
    "def extrapolate_nans(x, y, v):\n",
    "    if x.size == 0 or y.size == 0:\n",
    "        # nothing to do\n",
    "        return np.array([[]])\n",
    "    if np.ma.is_masked(v):\n",
    "        nans = v.mask\n",
    "    else:\n",
    "        nans = np.isnan(v)\n",
    "    notnans = np.logical_not(nans)\n",
    "    v[nans] = interpolate.griddata((x[notnans], y[notnans]), v[notnans],\n",
    "                                   (x[nans], y[nans]),\n",
    "                                   method='nearest', rescale=True)\n",
    "    return v\n",
    "\n",
    "def get_col_row(x, size=30):\n",
    "    ncols = da.ptp(x) / size\n",
    "    return int(ncols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6411e4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = SLURMCluster(queue=\"seseml\",\n",
    "                       memory='200GB',\n",
    "                       cores=40,\n",
    "                       processes=1,\n",
    "                       walltime='48:00:00',\n",
    "                       scheduler_options={'host': '172.22.179.3:7223', 'dashboard_address': ':7999'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcfdbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster.scale(2)\n",
    "cluster.adapt(maximum_jobs=2)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8829fd3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e543b03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_xr = xr.open_zarr(f'{path_data}/zarr_rckd/KUsKAs_Wn/lores.zarr')\n",
    "ds_xr = ds_xr.sel(time=~ds_xr.get_index(\"time\").duplicated())\n",
    "data = ds_xr.sel(time='2019-09-16 03:12:58').isel(time=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2763bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.range *  data.DR.values * np.sin(np.deg2rad(data.azimuth))  # add roll\n",
    "y = data.alt3d.values\n",
    "z =  data.zhh14.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8887fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ncols = get_col_row(x=x, size=30)\n",
    "nrows = get_col_row(x=y, size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a403e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new = np.linspace(x.min(), x.max(), int(ncols))\n",
    "y_new = np.linspace(y.max(), y.min(), int(nrows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2fcecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "gd1 = dask_gd2(x.values, y.values, z, x_new, y_new, algorithm='linear', n_jobs=8)\n",
    "plt.figure()\n",
    "plt.imshow(gd1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7012ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test2\n",
    "ar_size_x, ar_size_y = 500, 600\n",
    "chunk_size = 100\n",
    "z_array = np.ones((ar_size_x, ar_size_y))\n",
    "\n",
    "# XY coords\n",
    "x = np.linspace(0, 3, z_array.shape[1])\n",
    "y = np.linspace(0, 3, z_array.shape[0])\n",
    "xx, yy = np.meshgrid(x, y)\n",
    "\n",
    "# gen sin wave for testing\n",
    "z_array = z_array * np.sin(x)\n",
    "\n",
    "# target grid\n",
    "x_target = np.linspace(0, 3, 200)\n",
    "y_target = np.linspace(0, 3, 100)\n",
    "\n",
    "gd1 = dask_gd2(xx, yy, z_array, x_target, y_target, algorithm='cubic', n_jobs=8)\n",
    "plt.figure()\n",
    "plt.imshow(gd1)\n",
    "# plt.savefig(\"dask_par_sin_t2.png\")\n",
    "# plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda216ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(z_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92afb20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @jit\n",
    "def get_col_row(x, size=30):\n",
    "    ncols = da.ptp(x) / size\n",
    "    return int(ncols)\n",
    "\n",
    "\n",
    "def excluding_mesh(x, y, nx=30, ny=30):\n",
    "    \"\"\"\n",
    "    Construct a grid of points, that are some distance away from points (x,\n",
    "    \"\"\"\n",
    "\n",
    "    dx = x.ptp() / nx\n",
    "    dy = y.ptp() / ny\n",
    "\n",
    "    xp, yp = np.mgrid[x.min() - 2 * dx:x.max() + 2 * dx:(nx + 2) * 1j,\n",
    "             y.min() - 2 * dy:y.max() + 2 * dy:(ny + 2) * 1j]\n",
    "    xp = xp.ravel()\n",
    "    yp = yp.ravel()\n",
    "\n",
    "    tree = KDTree(np.c_[x, y])\n",
    "    dist, j = tree.query(np.c_[xp, yp], k=1)\n",
    "\n",
    "    # Select points sufficiently far away\n",
    "    m = (dist > np.hypot(dx, dy))\n",
    "    return xp[m], yp[m]\n",
    "\n",
    "\n",
    "def regridd(data, x, y, size=30):\n",
    "    \"\"\"\n",
    "    data = xarray datarray\n",
    "    size = desired pixel size in meters\n",
    "    \"\"\"\n",
    "    if data.ndim > 2:\n",
    "        x = da.moveaxis(x.reshape(-1, x.shape[-1]), 0, -1)\n",
    "        y = da.moveaxis(y.reshape(-1, y.shape[-1]), 0, -1)\n",
    "        ncols_n = max(np.apply_along_axis(get_col_row, arr=x, axis=1))\n",
    "        nrows_n = max(np.apply_along_axis(get_col_row, arr=y, axis=1))\n",
    "        x_new_n = da.from_array(np.moveaxis(np.linspace(np.amin(x, -1), np.amax(x, -1), ncols_n), 0, -1))\n",
    "        y_new_n = da.from_array(np.moveaxis(np.linspace(np.amax(y, -1), np.amin(y, -1), nrows_n), 0, -1))\n",
    "        mesh = [delayed(da.meshgrid)(x_new_n[i], y_new_n[i]) for i in range(x_new_n.shape[0])]\n",
    "\n",
    "        z_n = da.rollaxis(data.reshape(-1, data.shape[-1]), 1)\n",
    "        idx_n = x.argsort(axis=-1)\n",
    "        x = np.take_along_axis(x, idx_n, axis=-1)\n",
    "        y = np.take_along_axis(y, idx_n, axis=-1)\n",
    "        z_n = np.take_along_axis(z_n, idx_n, axis=-1)\n",
    "\n",
    "        vp_n = dask.compute(*[delayed(excluding_mesh)(x[i], y[i]) for i in range(x.shape[0])])\n",
    "        xn = [vp_n[i][0] for i in range(len(vp_n))]\n",
    "        yn = [vp_n[i][1] for i in range(len(vp_n))]\n",
    "        zn = dask.compute(*[delayed(da.zeros_like)(xn[i]) for i in range(x.shape[0])])\n",
    "        xi_ = [mesh[i][0] for i in range(len(vp_n))]\n",
    "        xi_ = dask.compute(*[da.from_delayed(v, shape=(x.shape[0], np.nan), dtype=float) for v in xi_])\n",
    "        yi_ = [mesh[i][1] for i in range(len(vp_n))]\n",
    "        yi_ = dask.compute(*[da.from_delayed(v, shape=(x.shape[0], np.nan), dtype=float) for v in yi_])\n",
    "        zr = [delayed(griddata)((np.r_[x[i, :], xn[i]], np.r_[y[i, :], yn[i]]), np.r_[z_n[i, :], zn[i]],\n",
    "                                (xi_[i], yi_[i]), method='linear', fill_value=0)\n",
    "              for i in range(x.shape[0])]\n",
    "        zr = da.dstack(dask.compute(*zr))\n",
    "        xi_ = da.rollaxis(da.rollaxis(da.asarray(xi_), axis=-1), axis=-1)\n",
    "        yi_ = da.rollaxis(da.rollaxis(da.asarray(yi_), axis=-1), axis=-1)\n",
    "        return zr, xi_, yi_\n",
    "\n",
    "    else:\n",
    "        x_s = x.flatten()\n",
    "        y_s = y.flatten()\n",
    "        data = data.compute().flatten()\n",
    "        idx = x_s.argsort()\n",
    "        x_s, y_s = np.take_along_axis(x_s, idx, axis=0), np.take_along_axis(y_s, idx, axis=0)\n",
    "        data = np.take_along_axis(data, idx, axis=0)\n",
    "        ncols = get_col_row(x=x_s, size=size)\n",
    "        nrows = get_col_row(x=y_s, size=size)\n",
    "        x_new = np.linspace(x_s.min(), x_s.max(), int(ncols))\n",
    "        y_new = np.linspace(y_s.max(), y_s.min(), int(nrows))\n",
    "        xi, yi = np.meshgrid(x_new, y_new)\n",
    "        xp, yp = excluding_mesh(x_s, y_s, nx=35, ny=35)\n",
    "        zp = np.nan + np.zeros_like(xp)\n",
    "        z0 = griddata((np.r_[x_s, xp], np.r_[y_s, yp]), np.r_[data, zp], (xi, yi), method='linear', fill_value=-9999)\n",
    "        return z0, xi, yi\n",
    "\n",
    "\n",
    "def lee_filter_new(img, size, tresh=-150):\n",
    "    if img.ndim == 2:\n",
    "        shape = (size, size)\n",
    "    else:\n",
    "        shape = (size, size, 1)\n",
    "    img = da.where(da.logical_or(da.isnan(img), da.equal(img, -9999)), tresh, img)\n",
    "    img_mean = uf(img, shape)\n",
    "    img_sqr_mean = uf(da.power(img, 2), shape)\n",
    "    img_variance = img_sqr_mean - da.power(img_mean, 2)\n",
    "    overall_variance = varian(img)\n",
    "    img_weights = img_variance / (img_variance + overall_variance)\n",
    "    img_output = img_mean + img_weights * (img - img_mean)\n",
    "    img_output = da.where(img_output > 0, img_output, 0)\n",
    "    return img_output\n",
    "\n",
    "\n",
    "def process_new(zhh14, x, y, time):\n",
    "    x = x[:, 0, :, :]\n",
    "    img_filtered = lee_filter_new(zhh14, size=3, tresh=-200)\n",
    "    img, xi, yi = regridd(img_filtered, x, y)\n",
    "    px_tot, _, _ = regridd(np.ones_like(zhh14), x, y)\n",
    "    px_tot = np.where(px_tot < 0.99, 0, px_tot)\n",
    "    px_tot = np.apply_along_axis(np.count_nonzero, arr=px_tot, axis=1)\n",
    "    num_px = np.apply_along_axis(np.count_nonzero, arr=img, axis=1)\n",
    "    img = np.where(img > 0., img, 0.)\n",
    "    blurred = gaussian(img, sigma=0.8)\n",
    "    binary = blurred > threshold_otsu(blurred)\n",
    "    labels = measure.label(binary)\n",
    "    if (labels.ndim > 2) & (zhh14.shape[-1] != 1):\n",
    "        max_zhh14 = np.apply_along_axis(np.max, arr=img, axis=0).compute()\n",
    "        max_zhh14 = np.apply_along_axis(np.max, arr=max_zhh14, axis=0)\n",
    "        df_max_zhh = pd.Series(max_zhh14, index=time, name='max_zhh')\n",
    "        df_num_px = pd.DataFrame(np.moveaxis(num_px.compute(), 0, -1), index=time)\n",
    "        df_num_px.columns = pd.MultiIndex.from_product([['num_px'], df_num_px.columns])\n",
    "        df_tot = pd.DataFrame(np.moveaxis(px_tot.compute(), 0, -1), index=time)\n",
    "        df_tot.columns = pd.MultiIndex.from_product([['tot_px'], df_tot.columns])\n",
    "        tab_props = [measure.regionprops_table(labels[:, :, i], img[:, :, i],\n",
    "                                               properties=['area', 'perimeter', 'bbox', 'major_axis_length',\n",
    "                                                           'minor_axis_length']) for i in range(labels.shape[-1])]\n",
    "\n",
    "        df = pd.DataFrame(data=tab_props, index=pd.to_datetime(time))\n",
    "        df = df.merge(df_max_zhh, left_index=True, right_index=True)\n",
    "        df = df.merge(df_num_px, left_index=True, right_index=True)\n",
    "        df = df.merge(df_tot, left_index=True, right_index=True)\n",
    "    else:\n",
    "        max_zhh = np.max(img.compute())\n",
    "        props = measure.regionprops_table(labels[:, :, 0], img[:, :, 0].compute(),\n",
    "                                          properties=['area', 'perimeter', 'bbox', 'major_axis_length',\n",
    "                                                      'minor_axis_length'])\n",
    "        props['max_zhh'] = max_zhh\n",
    "        df = pd.DataFrame(data=props, index=pd.to_datetime(time))\n",
    "\n",
    "    dates = datetime.now(timezone.utc)\n",
    "    df.to_csv(f\"../results/all_filtered_{dates:%Y}{dates:%m}{dates:%d}{dates:%H}{dates:%M}.csv\")\n",
    "    return 'done!'\n",
    "\n",
    "\n",
    "def process_plot(zhh14, zhh35, _range, azimuth, alt3d, bin_size, time):\n",
    "    x = _range * bin_size.values[0] * np.sin(np.deg2rad(azimuth))  # add roll\n",
    "    y = alt3d\n",
    "    img_filtered = lee_filter_new(zhh14, size=3, tresh=-180)\n",
    "    img_filtered_zhh35 = lee_filter_new(zhh35, size=3, tresh=-180)\n",
    "    \n",
    "    img, xi, yi = regridd(img_filtered, x.values, y.values)\n",
    "    img_35, xi_35, yi_35 = regridd(img_filtered_zhh35, x.values, y.values)\n",
    "    \n",
    "    img = np.where(img > 0., img, 0.)\n",
    "    img_35 = np.where(img_35 > 0., img_35, 0.)\n",
    "    \n",
    "    blurred = gaussian(img, sigma=0.8)\n",
    "    blurred_35 = gaussian(img_35, sigma=0.8)\n",
    "    \n",
    "    binary = blurred > threshold_otsu(blurred)\n",
    "    binary_35 = blurred_35 > threshold_otsu(blurred_35)\n",
    "    \n",
    "    labels = measure.label(binary)\n",
    "    labels_35 = measure.label(binary_35)\n",
    "    \n",
    "    if labels.ndim > 2:\n",
    "        props = [measure.regionprops(labels[:, :, i]) for i in range(labels.shape[-1])]\n",
    "\n",
    "        \n",
    "        props_35 = [measure.regionprops(labels_35[:, :, i]) for i in range(labels_35.shape[-1])]\n",
    "\n",
    "        \n",
    "    else:\n",
    "        props = measure.regionprops(labels)\n",
    "\n",
    "        \n",
    "        props_35 = measure.regionprops(labels_35)\n",
    "    \n",
    "\n",
    "    if img.ndim > 2:\n",
    "        shp = img.shape[-1]\n",
    "        for i in range(shp):\n",
    "            fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(18, 8))\n",
    "            img_plot = img[:, :, i]\n",
    "            img_plot_35 = img_35[:, :, i]\n",
    "            \n",
    "            ax1.pcolormesh(x.isel(time=i), y.isel(time=i), img_filtered.compute()[:, :, i], cmap='jet',\n",
    "                           vmax=50, vmin=0, shading='auto')\n",
    "            ax2.imshow(img_plot, aspect='auto', cmap='jet', vmax=50, vmin=0)\n",
    "#             ax2.pcolormesh(xi[:, :, i], yi[:, :, i], img_plot, cmap='jet',\n",
    "#                            vmax=50, vmin=0, shading='auto')\n",
    "            \n",
    "            ax3.pcolormesh(x.isel(time=i), y.isel(time=i), img_filtered_zhh35.compute()[:, :, i], cmap='jet',\n",
    "                           vmax=50, vmin=0, shading='auto')\n",
    "            ax4.imshow(img_plot_35, aspect='auto', cmap='jet', vmax=50, vmin=0)\n",
    "#             ax4.pcolormesh(xi[:, :, i], yi[:, :, i], img_plot_35, cmap='jet',\n",
    "#                            vmax=50, vmin=0, shading='auto')\n",
    "            \n",
    "            for region in props[i]:\n",
    "                if region.area >= 100:\n",
    "                    minr, minc, maxr, maxc = region.bbox\n",
    "                    rect = mpatches.Rectangle((minc, minr), maxc - minc, maxr - minr,\n",
    "                                              fill=False, edgecolor='red', linewidth=2)\n",
    "                    ax2.add_patch(rect)\n",
    "            \n",
    "            for region in props_35[i]:\n",
    "                if region.area >= 100:\n",
    "                    minr, minc, maxr, maxc = region.bbox\n",
    "                    rect = mpatches.Rectangle((minc, minr), maxc - minc, maxr - minr,\n",
    "                                              fill=False, edgecolor='red', linewidth=2)\n",
    "                    ax4.add_patch(rect)\n",
    "            \n",
    "            \n",
    "            plt.show()\n",
    "            print('a')\n",
    "\n",
    "    else:\n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 10))\n",
    "        img_plot = img\n",
    "        ax1.pcolormesh(x, y, img_filtered.compute(), cmap='jet', vmax=60, vmin=0, shading='auto')\n",
    "        ax2.imshow(img_plot, aspect='auto', cmap='jet', vmax=60, vmin=0)\n",
    "        ax3.pcolormesh(xi, yi, img_plot, cmap='jet', vmax=60, vmin=0)\n",
    "        ax3.set_yticks(np.arange(yi.min(), yi.max(), 250))\n",
    "        for region in props:\n",
    "            if region.area >= 100:\n",
    "                minr, minc, maxr, maxc = region.bbox\n",
    "                rect = mpatches.Rectangle((minc, minr), maxc - minc, maxr - minr,\n",
    "                                          fill=False, edgecolor='red', linewidth=2)\n",
    "                ax2.add_patch(rect)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db2cb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds_xr = xr.open_zarr(f'{path_data}/zarr_rckd/KUsKAs_Wn/lores.zarr')\n",
    "# ds_xr = xr.open_zarr(f'{path_data}/zarr/KUsKAs_Wn/lores.zarr')\n",
    "ds_xr = ds_xr.sel(time=~ds_xr.get_index(\"time\").duplicated())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5871a786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_data = ds_xr[['zhh14', 'azimuth', 'DR']].isel(time=range(50,60))\n",
    "ds_data = ds_xr[['zhh14', 'zhh35', 'azimuth', 'DR']].sel(time=slice('2019-09-16 03:12:50', '2019-09-16 03:13:05'))\n",
    "# ds_data = ds_xr[['zhh14', 'azimuth', 'DR']].isel(time=slice(178, 190))\n",
    "ds_zhh = ds_data.zhh14.where(ds_data.alt3d > 500)\n",
    "ds_zhh35 = ds_data.zhh35.where(ds_data.alt3d > 500)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1bb6f1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "process_plot(zhh14=ds_zhh, zhh35=ds_zhh35, _range=ds_data.range, azimuth=ds_data.azimuth,\n",
    "            alt3d=ds_data.alt3d, bin_size=ds_data.DR, time=ds_data.time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b855b95b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae3de81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:camp2ex_proj] *",
   "language": "python",
   "name": "conda-env-camp2ex_proj-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
